{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Mimi\n",
    "\n",
    "Audio codec of Moshi (low latency Audio LM): https://huggingface.co/kyutai/mimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "import torch\n",
    "from transformers import MimiModel, AutoFeatureExtractor\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = MimiModel.from_pretrained(\"kyutai/mimi\").cuda()\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anaysis synthesis\n",
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "device=torch.device('cuda')\n",
    "\n",
    "wav_path = os.path.join(\"speech_gen_eval_testsets\", \"vctk\", \"wav\", \"p225_011.wav\")\n",
    "wav, _ = librosa.load(wav_path, sr=24000)\n",
    "display(Audio(wav, rate=24000))\n",
    "\n",
    "inputs = feature_extractor(\n",
    "    raw_audio=wav,\n",
    "    sampling_rate=feature_extractor.sampling_rate,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "encoder_outputs = model.encode(inputs[\"input_values\"].cuda())\n",
    "tokens = encoder_outputs.audio_codes[:, :8, :]  # using first 8 tokens\n",
    "plt.imshow(tokens[0].detach().cpu().numpy(), aspect=\"auto\")\n",
    "plt.show()\n",
    "# reconstruct\n",
    "wav = model.decode(tokens)[0]\n",
    "display(Audio(wav[0][0].detach().cpu().numpy(), rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# for testsets do analysis synthesis and save files to a directory for evaluation\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "out_dir = \"mimi\"\n",
    "device=torch.device('cuda')\n",
    "\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    out_fold_dir = os.path.join(out_dir, fold, \"wav\")\n",
    "    os.makedirs(out_fold_dir, exist_ok=True)\n",
    "    with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"test\"), \"r\") as fp:\n",
    "        for line in tqdm.tqdm(fp):\n",
    "            id, txt = line.strip().split(\"\\t\", 1)\n",
    "            out_path = os.path.join(out_fold_dir, id + \".wav\")\n",
    "            if os.path.exists(out_path):\n",
    "                continue\n",
    "            in_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\", id + \".wav\")\n",
    "            wav, _ = librosa.load(in_path, sr=24000)\n",
    "            inputs = feature_extractor(\n",
    "                raw_audio=wav,\n",
    "                sampling_rate=feature_extractor.sampling_rate,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            encoder_outputs = model.encode(inputs[\"input_values\"].cuda())\n",
    "            tokens = encoder_outputs.audio_codes[:, :8, :]  # using first 8 tokens\n",
    "            # reconstruct\n",
    "            wav = model.decode(tokens)[0]\n",
    "            sf.write(out_path, wav[0][0].detach().cpu().numpy(), 24000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally run evaluation with `speech_gen_eval`\n",
    "import os\n",
    "from speech_gen_eval.evaluation import speech_gen_eval\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    print(f\"Evaluating {fold}\")\n",
    "    txt_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"test\")\n",
    "    generated_audio = os.path.join(\"mimi\", fold, \"wav\")\n",
    "    original_audio = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\")\n",
    "    speech_gen_eval(\n",
    "        txt_path=txt_path,\n",
    "        generated_audio=generated_audio,\n",
    "        eval_type=\"vocoder\",\n",
    "        original_audio=original_audio,\n",
    "        out_path=os.path.join(\"mimi\", fold, \"metrics.yaml\"),\n",
    "        ignore_missing=True,\n",
    "        # extra arguments to write into the metrics.yaml as meta info\n",
    "        model_name=\"Mimi(8books)\",\n",
    "        dataset=f\"balacoon/speech_gen_eval_testsets/{fold}\",\n",
    "        link=\"https://huggingface.co/kyutai/mimi\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont upload all the files from test (2k),\n",
    "# but only those (150) that are meant to be kept for listening / subjective evaluation\n",
    "import os\n",
    "import glob\n",
    "\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Read keep file and get IDs\n",
    "    keep_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"keep\")\n",
    "    with open(keep_path, encoding=\"utf-8\") as f:\n",
    "        keep_lines = f.readlines()\n",
    "    keep_ids = set([line.split()[0] for line in keep_lines])\n",
    "    \n",
    "    for wav_file in glob.glob(os.path.join(\"mimi\", fold, \"wav\", \"*.wav\")):\n",
    "        if os.path.basename(wav_file).split(\".\")[0] not in keep_ids:\n",
    "            os.remove(wav_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload synthetic audio and metrics to `speech_gen_baselines`,\n",
    "# so it is available on TTSLeaderboard\n",
    "\n",
    "local_dataset = \"mimi\"\n",
    "hf_dataset = \"balacoon/speech_gen_baselines\"\n",
    "hf_subdir = \"vocoder/mimi\"\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Upload each fold to the appropriate subdirectory\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Upload wav files\n",
    "    api.upload_folder(\n",
    "        folder_path=os.path.join(local_dataset, fold),\n",
    "        repo_id=hf_dataset,\n",
    "        repo_type=\"dataset\",\n",
    "        path_in_repo=os.path.join(hf_subdir, fold)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_gen_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
