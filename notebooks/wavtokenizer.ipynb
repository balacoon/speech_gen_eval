{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate WavTokenizer\n",
    "\n",
    "THe first discrete audio codec compressing the audio into a single stream of tokens: https://github.com/jishengpeng/WavTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to resolve issues with fairseq manually\n",
    "# downloaded repo, put configs, encoder and decoder dirs in the current di\n",
    "!pip install -r https://raw.githubusercontent.com/jishengpeng/WavTokenizer/main/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Define model repo and filename\n",
    "repo_id = \"novateur/WavTokenizer-large-speech-75token\"\n",
    "filename = \"wavtokenizer_large_speech_320_v2.ckpt\"\n",
    "\n",
    "# Download the model directly into the current directory\n",
    "model_path = hf_hub_download(repo_id=repo_id, filename=filename, local_dir=\".\")\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "import torch\n",
    "from decoder.pretrained import WavTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "config_path = \"./configs/wavtokenizer_smalldata_frame75_3s_nq1_code4096_dim512_kmeans200_attn.yaml\"  # Replace 'xxx.yaml' with the correct config file name\n",
    "model_path = \"./wavtokenizer_large_speech_320_v2.ckpt\"\n",
    "\n",
    "wavtokenizer = WavTokenizer.from_pretrained0802(config_path, model_path)\n",
    "wavtokenizer = wavtokenizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anaysis synthesis\n",
    "\n",
    "from encoder.utils import convert_audio\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pylab as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "device=torch.device('cuda')\n",
    "\n",
    "wav_path = os.path.join(\"speech_gen_eval_testsets\", \"vctk\", \"wav\", \"p225_011.wav\")\n",
    "wav, sr = torchaudio.load(wav_path)\n",
    "wav = convert_audio(wav, sr, 24000, 1)\n",
    "display(Audio(wav, rate=24000))\n",
    "bandwidth_id = torch.tensor([0])\n",
    "wav=wav.to(device)\n",
    "_, discrete_code= wavtokenizer.encode_infer(wav, bandwidth_id=bandwidth_id)\n",
    "plt.plot(discrete_code[0][0].detach().cpu().numpy())\n",
    "plt.show()\n",
    "# reconstruct\n",
    "features = wavtokenizer.codes_to_features(discrete_code)\n",
    "bandwidth_id = torch.tensor([0], device=device)  \n",
    "audio_out = wavtokenizer.decode(features, bandwidth_id=bandwidth_id)\n",
    "display(Audio(audio_out[0].detach().cpu().numpy(), rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import soundfile as sf\n",
    "\n",
    "# for testsets do analysis synthesis and save files to a directory for evaluation\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "out_dir = \"wavtokenizer\"\n",
    "device=torch.device('cuda')\n",
    "\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    out_fold_dir = os.path.join(out_dir, fold, \"wav\")\n",
    "    os.makedirs(out_fold_dir, exist_ok=True)\n",
    "    with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"test\"), \"r\") as fp:\n",
    "        for line in tqdm.tqdm(fp):\n",
    "            id, txt = line.strip().split(\"\\t\", 1)\n",
    "            out_path = os.path.join(out_fold_dir, id + \".wav\")\n",
    "            if os.path.exists(out_path):\n",
    "                continue\n",
    "            in_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\", id + \".wav\")\n",
    "\n",
    "            wav, sr = torchaudio.load(in_path)\n",
    "            wav = convert_audio(wav, sr, 24000, 1)\n",
    "            bandwidth_id = torch.tensor([0], device=device)\n",
    "            wav=wav.to(device)\n",
    "            _, discrete_code= wavtokenizer.encode_infer(wav, bandwidth_id=bandwidth_id)\n",
    "            # reconstruct\n",
    "            features = wavtokenizer.codes_to_features(discrete_code)\n",
    "            audio_out = wavtokenizer.decode(features, bandwidth_id=bandwidth_id)\n",
    "            sf.write(out_path, audio_out[0].detach().cpu().numpy(), 24000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally run evaluation with `speech_gen_eval`\n",
    "import os\n",
    "from speech_gen_eval.evaluation import speech_gen_eval\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    print(f\"Evaluating {fold}\")\n",
    "    txt_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"test\")\n",
    "    generated_audio = os.path.join(\"wavtokenizer\", fold, \"wav\")\n",
    "    original_audio = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\")\n",
    "    speech_gen_eval(\n",
    "        txt_path=txt_path,\n",
    "        generated_audio=generated_audio,\n",
    "        eval_type=\"vocoder\",\n",
    "        original_audio=original_audio,\n",
    "        out_path=os.path.join(\"wavtokenizer\", fold, \"metrics.yaml\"),\n",
    "        ignore_missing=True,\n",
    "        # extra arguments to write into the metrics.yaml as meta info\n",
    "        model_name=\"WavTokenizer(LargeV2)\",\n",
    "        dataset=f\"balacoon/speech_gen_eval_testsets/{fold}\",\n",
    "        link=\"https://github.com/jishengpeng/WavTokenizer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont upload all the files from test (2k),\n",
    "# but only those (150) that are meant to be kept for listening / subjective evaluation\n",
    "import os\n",
    "import glob\n",
    "\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Read keep file and get IDs\n",
    "    keep_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"keep\")\n",
    "    with open(keep_path, encoding=\"utf-8\") as f:\n",
    "        keep_lines = f.readlines()\n",
    "    keep_ids = set([line.split()[0] for line in keep_lines])\n",
    "    \n",
    "    for wav_file in glob.glob(os.path.join(\"wavtokenizer\", fold, \"wav\", \"*.wav\")):\n",
    "        if os.path.basename(wav_file).split(\".\")[0] not in keep_ids:\n",
    "            os.remove(wav_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload synthetic audio and metrics to `speech_gen_baselines`,\n",
    "# so it is available on TTSLeaderboard\n",
    "\n",
    "local_dataset = \"wavtokenizer\"\n",
    "hf_dataset = \"balacoon/speech_gen_baselines\"\n",
    "hf_subdir = \"vocoder/wavtokenizer\"\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Upload each fold to the appropriate subdirectory\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Upload wav files\n",
    "    api.upload_folder(\n",
    "        folder_path=os.path.join(local_dataset, fold),\n",
    "        repo_id=hf_dataset,\n",
    "        repo_type=\"dataset\",\n",
    "        path_in_repo=os.path.join(hf_subdir, fold)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_gen_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
