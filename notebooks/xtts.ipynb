{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate XTTSv2\n",
    "\n",
    "download `balacoon/speech_gen_eval_testsets`, run inference with xtts, run evaluation, upload results to leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "if not os.path.isdir(\"speech_gen_eval_testsets\"):\n",
    "    snapshot_download(\n",
    "        repo_id=\"balacoon/speech_gen_eval_testsets\",\n",
    "        local_dir=\"speech_gen_eval_testsets\",\n",
    "        repo_type=\"dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "setting up xtts environment:\n",
    "```bash\n",
    "conda create -n xtts python=3.10\n",
    "conda activate xtts\n",
    "pip install git+https://github.com/coqui-ai/TTS\n",
    "# had to downgrade torch\n",
    "pip install torch==2.1 torchaudio==2.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# https://github.com/coqui-ai/TTS/blob/dev/docs/source/models/xtts.md#single-reference-1\n",
    "import os\n",
    "from TTS.api import TTS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# run the synthesis for the vctk\n",
    "fold = \"vctk\"\n",
    "mapping = {}\n",
    "out_dir = os.path.join(\"xtts\", fold, \"wav\")\n",
    "ref_dir = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"id_mapping\"), \"r\") as fp:\n",
    "    for line in fp:\n",
    "        k, v = line.strip().split()\n",
    "        mapping[k] = v\n",
    "with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"test\"), \"r\") as fp:\n",
    "    for line in tqdm.tqdm(fp):\n",
    "        id, txt = line.strip().split(\"\\t\", 1)\n",
    "        out_path = os.path.join(out_dir, id + \".wav\")\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "        tts.tts_to_file(\n",
    "            text=txt,\n",
    "            file_path=out_path,\n",
    "            speaker_wav=[os.path.join(ref_dir, mapping[id] + \".wav\")],\n",
    "            split_sentences=False,\n",
    "            language=\"en\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import re\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# run the synthesis for the daps_celeb\n",
    "# unfortunately, the model can only generate texts up to 250 characters.\n",
    "# daps_celeb contains some texts that are longer than 250 characters.\n",
    "# for those lines we split text on closest punctiation mark and generate audio in chunks\n",
    "fold = \"daps_celeb\"\n",
    "mapping = {}\n",
    "out_dir = os.path.join(\"xtts\", fold, \"wav\")\n",
    "ref_dir = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"id_mapping\"), \"r\") as fp:\n",
    "    for line in fp:\n",
    "        k, v = line.strip().split()\n",
    "        mapping[k] = v\n",
    "with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"test\"), \"r\") as fp:\n",
    "    for line in tqdm.tqdm(fp):\n",
    "        id, txt = line.strip().split(\"\\t\", 1)\n",
    "        out_path = os.path.join(out_dir, id + \".wav\")\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "        if len(txt) < 250:\n",
    "            tts.tts_to_file(\n",
    "                text=txt,\n",
    "                file_path=out_path,\n",
    "                speaker_wav=[os.path.join(ref_dir, mapping[id] + \".wav\")],\n",
    "                split_sentences=False,\n",
    "                language=\"en\"\n",
    "            )\n",
    "        else:\n",
    "            # Split on punctuation marks\n",
    "            chunks = []\n",
    "            current_chunk = \"\"\n",
    "            words = txt.split()\n",
    "            \n",
    "            for word in words:\n",
    "                if len(current_chunk + \" \" + word) < 250:\n",
    "                    current_chunk += \" \" + word if current_chunk else word\n",
    "                else:\n",
    "                    # Find last punctuation mark in current chunk\n",
    "                    punct_matches = list(re.finditer(r'[,.!?;]', current_chunk))\n",
    "                    if punct_matches:\n",
    "                        split_idx = punct_matches[-1].end()\n",
    "                        chunks.append(current_chunk[:split_idx].strip())\n",
    "                        current_chunk = current_chunk[split_idx:].strip() + \" \" + word\n",
    "                    else:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                        current_chunk = word\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            \n",
    "            # Generate audio for each chunk\n",
    "            chunk_audios = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_path = f\"{out_path}.chunk{i}.wav\"\n",
    "                tts.tts_to_file(\n",
    "                    text=chunk,\n",
    "                    file_path=chunk_path,\n",
    "                    speaker_wav=[os.path.join(ref_dir, mapping[id] + \".wav\")],\n",
    "                    split_sentences=False,\n",
    "                    language=\"en\"\n",
    "                )\n",
    "                audio, sr = sf.read(chunk_path)\n",
    "                chunk_audios.append(audio)\n",
    "                os.remove(chunk_path)  # Delete chunk file\n",
    "            \n",
    "            # Concatenate and save final audio\n",
    "            final_audio = np.concatenate(chunk_audios)\n",
    "            sf.write(out_path, final_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally run evaluation with `speech_gen_eval`\n",
    "import os\n",
    "from speech_gen_eval.evaluation import speech_gen_eval\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    print(f\"Evaluating {fold}\")\n",
    "    txt_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"test\")\n",
    "    generated_audio = os.path.join(\"xtts\", fold, \"wav\")\n",
    "    original_audio = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\")\n",
    "    id_mapping = os.path.join(\"speech_gen_eval_testsets\", fold, \"id_mapping\")\n",
    "    speech_gen_eval(\n",
    "        txt_path=txt_path,\n",
    "        generated_audio=generated_audio,\n",
    "        eval_type=\"zero-tts\",\n",
    "        original_audio=original_audio,\n",
    "        mapping_path=id_mapping,\n",
    "        out_path=os.path.join(\"xtts\", fold, \"metrics.yaml\"),\n",
    "        ignore_missing=True,\n",
    "        # extra arguments to write into the metrics.yaml as meta info\n",
    "        model_name=\"XTTSv2\",\n",
    "        dataset=f\"balacoon/speech_gen_eval_testsets/{fold}\",\n",
    "        link=\"https://github.com/coqui-ai/TTS/blob/dev/docs/source/models/xtts.md#single-reference-1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont upload all the files from test (2k),\n",
    "# but only those (150) that are meant to be kept for listening / subjective evaluation\n",
    "import os\n",
    "import glob\n",
    "\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Read keep file and get IDs\n",
    "    keep_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"keep\")\n",
    "    with open(keep_path) as f:\n",
    "        keep_lines = f.readlines()\n",
    "    keep_ids = set([line.split()[0] for line in keep_lines])\n",
    "    \n",
    "    for wav_file in glob.glob(os.path.join(\"xtts\", fold, \"wav\", \"*.wav\")):\n",
    "        if os.path.basename(wav_file).split(\".\")[0] not in keep_ids:\n",
    "            os.remove(wav_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload synthetic audio and metrics to `speech_gen_baselines`,\n",
    "# so it is available on TTSLeaderboard\n",
    "\n",
    "local_dataset = \"xtts\"\n",
    "hf_dataset = \"balacoon/speech_gen_baselines\"\n",
    "hf_subdir = \"zero-tts/xtts\"\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Upload each fold to the appropriate subdirectory\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Upload wav files\n",
    "    api.upload_folder(\n",
    "        folder_path=os.path.join(local_dataset, fold),\n",
    "        repo_id=hf_dataset,\n",
    "        repo_type=\"dataset\",\n",
    "        path_in_repo=os.path.join(hf_subdir, fold)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vctk and daps_celeb in speech_gen_eval_testsets,\n",
    "# read test and demo, take demo, then 140 more other random lines from test\n",
    "# and write it all to \"keep\" next to demo.\n",
    "import random\n",
    "import os\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Read demo file\n",
    "    demo_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"demo\")\n",
    "    with open(demo_path) as f:\n",
    "        demo_lines = f.readlines()\n",
    "    \n",
    "    # Read test file\n",
    "    test_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"test\") \n",
    "    with open(test_path) as f:\n",
    "        test_lines = f.readlines()\n",
    "        \n",
    "    # Remove demo lines from test lines to avoid duplicates\n",
    "    test_lines_unique = [line for line in test_lines if line not in demo_lines]\n",
    "        \n",
    "    # Sample 140 random lines from remaining test lines\n",
    "    sampled_test = random.sample(test_lines_unique, 140)\n",
    "    \n",
    "    # Combine demo and sampled test lines\n",
    "    keep_lines = demo_lines + sampled_test\n",
    "    \n",
    "    # Write to keep file\n",
    "    keep_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"keep\")\n",
    "    with open(keep_path, \"w\") as f:\n",
    "        f.writelines(keep_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# Upload keep files to HuggingFace dataset\n",
    "api = HfApi()\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    keep_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"keep\")\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=keep_path,\n",
    "        path_in_repo=os.path.join(fold, \"keep\"),\n",
    "        repo_id=\"balacoon/speech_gen_eval_testsets\",\n",
    "        repo_type=\"dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/balacoon/speech_gen_baselines/commit/main (Request ID: Root=1-67b30d70-3d327b9f658834c17a670180;cd4ffa67-d2a4-40ac-9c5b-a706c1402a27)\n\nYou have been rate-limited; you can retry this action in about 1 hour. If you're a new user, your limits will raise progressively over time. Get in touch with us at website@huggingface.co if you need access now.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/speech_gen_eval/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/speech_gen_eval/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/balacoon/speech_gen_baselines/commit/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Delete files in bulk\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m files_to_delete:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelete_patterns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles_to_delete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalacoon/speech_gen_baselines\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#print(f\"Deleted {len(files_to_delete)} files from {fold}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech_gen_eval/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech_gen_eval/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4861\u001b[0m, in \u001b[0;36mHfApi.delete_files\u001b[0;34m(self, repo_id, delete_patterns, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit)\u001b[0m\n\u001b[1;32m   4858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4859\u001b[0m     commit_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDelete files \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(delete_patterns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4865\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4867\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4868\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4871\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech_gen_eval/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech_gen_eval/lib/python3.12/site-packages/huggingface_hub/hf_api.py:1524\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m-> 1524\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speech_gen_eval/lib/python3.12/site-packages/huggingface_hub/hf_api.py:4053\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   4052\u001b[0m     commit_resp \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39mcommit_url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mdata, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m-> 4053\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcommit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   4055\u001b[0m     e\u001b[38;5;241m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m~/miniconda3/envs/speech_gen_eval/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/balacoon/speech_gen_baselines/commit/main (Request ID: Root=1-67b30d70-3d327b9f658834c17a670180;cd4ffa67-d2a4-40ac-9c5b-a706c1402a27)\n\nYou have been rate-limited; you can retry this action in about 1 hour. If you're a new user, your limits will raise progressively over time. Get in touch with us at website@huggingface.co if you need access now."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Read keep file and get IDs\n",
    "    keep_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"keep\")\n",
    "    with open(keep_path) as f:\n",
    "        keep_lines = f.readlines()\n",
    "    keep_ids = [line.split()[0] for line in keep_lines]\n",
    "    \n",
    "    # Get list of files in wav directory from HF\n",
    "    wav_files = api.list_repo_tree(\n",
    "        repo_id=f\"balacoon/speech_gen_baselines\",\n",
    "        repo_type=\"dataset\",\n",
    "        path_in_repo=f\"zero-tts/xtts/{fold}/wav\"\n",
    "    )\n",
    "    wav_files = [x.path for x in wav_files]\n",
    "    \n",
    "    # Collect files to delete\n",
    "    files_to_delete = []\n",
    "    for file_path in wav_files:\n",
    "        if not file_path.endswith('.wav'):\n",
    "            continue\n",
    "            \n",
    "        file_id = os.path.basename(file_path).split(\".\")[0]\n",
    "        if file_id not in keep_ids:\n",
    "            files_to_delete.append(file_path)\n",
    "    \n",
    "    # Delete files in bulk\n",
    "    if files_to_delete:\n",
    "        api.delete_files(\n",
    "            delete_patterns=files_to_delete,\n",
    "            repo_id=\"balacoon/speech_gen_baselines\",\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        #print(f\"Deleted {len(files_to_delete)} files from {fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_gen_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
