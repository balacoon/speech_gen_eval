{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate XTTSv2\n",
    "\n",
    "download `balacoon/speech_gen_eval_testsets`, run inference with xtts, run evaluation, upload results to leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "if not os.path.isdir(\"speech_gen_eval_testsets\"):\n",
    "    snapshot_download(\n",
    "        repo_id=\"balacoon/speech_gen_eval_testsets\",\n",
    "        local_dir=\"speech_gen_eval_testsets\",\n",
    "        repo_type=\"dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "setting up xtts environment:\n",
    "```bash\n",
    "conda create -n xtts python=3.10\n",
    "conda activate xtts\n",
    "pip install git+https://github.com/coqui-ai/TTS\n",
    "# had to downgrade torch\n",
    "pip install torch==2.1 torchaudio==2.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# https://github.com/coqui-ai/TTS/blob/dev/docs/source/models/xtts.md#single-reference-1\n",
    "import os\n",
    "from TTS.api import TTS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "# run the synthesis for the vctk\n",
    "fold = \"vctk\"\n",
    "mapping = {}\n",
    "out_dir = os.path.join(\"xtts\", fold, \"wav\")\n",
    "ref_dir = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"id_mapping\"), \"r\") as fp:\n",
    "    for line in fp:\n",
    "        k, v = line.strip().split()\n",
    "        mapping[k] = v\n",
    "with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"test\"), \"r\") as fp:\n",
    "    for line in tqdm.tqdm(fp):\n",
    "        id, txt = line.strip().split(\"\\t\", 1)\n",
    "        out_path = os.path.join(out_dir, id + \".wav\")\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "        tts.tts_to_file(\n",
    "            text=txt,\n",
    "            file_path=out_path,\n",
    "            speaker_wav=[os.path.join(ref_dir, mapping[id] + \".wav\")],\n",
    "            split_sentences=False,\n",
    "            language=\"en\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import re\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# run the synthesis for the daps_celeb\n",
    "# unfortunately, the model can only generate texts up to 250 characters.\n",
    "# daps_celeb contains some texts that are longer than 250 characters.\n",
    "# for those lines we split text on closest punctiation mark and generate audio in chunks\n",
    "fold = \"daps_celeb\"\n",
    "mapping = {}\n",
    "out_dir = os.path.join(\"xtts\", fold, \"wav\")\n",
    "ref_dir = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"id_mapping\"), \"r\") as fp:\n",
    "    for line in fp:\n",
    "        k, v = line.strip().split()\n",
    "        mapping[k] = v\n",
    "with open(os.path.join(\"speech_gen_eval_testsets\", fold, \"test\"), \"r\") as fp:\n",
    "    for line in tqdm.tqdm(fp):\n",
    "        id, txt = line.strip().split(\"\\t\", 1)\n",
    "        out_path = os.path.join(out_dir, id + \".wav\")\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "        if len(txt) < 250:\n",
    "            tts.tts_to_file(\n",
    "                text=txt,\n",
    "                file_path=out_path,\n",
    "                speaker_wav=[os.path.join(ref_dir, mapping[id] + \".wav\")],\n",
    "                split_sentences=False,\n",
    "                language=\"en\"\n",
    "            )\n",
    "        else:\n",
    "            # Split on punctuation marks\n",
    "            chunks = []\n",
    "            current_chunk = \"\"\n",
    "            words = txt.split()\n",
    "            \n",
    "            for word in words:\n",
    "                if len(current_chunk + \" \" + word) < 250:\n",
    "                    current_chunk += \" \" + word if current_chunk else word\n",
    "                else:\n",
    "                    # Find last punctuation mark in current chunk\n",
    "                    punct_matches = list(re.finditer(r'[,.!?;]', current_chunk))\n",
    "                    if punct_matches:\n",
    "                        split_idx = punct_matches[-1].end()\n",
    "                        chunks.append(current_chunk[:split_idx].strip())\n",
    "                        current_chunk = current_chunk[split_idx:].strip() + \" \" + word\n",
    "                    else:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                        current_chunk = word\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            \n",
    "            # Generate audio for each chunk\n",
    "            chunk_audios = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_path = f\"{out_path}.chunk{i}.wav\"\n",
    "                tts.tts_to_file(\n",
    "                    text=chunk,\n",
    "                    file_path=chunk_path,\n",
    "                    speaker_wav=[os.path.join(ref_dir, mapping[id] + \".wav\")],\n",
    "                    split_sentences=False,\n",
    "                    language=\"en\"\n",
    "                )\n",
    "                audio, sr = sf.read(chunk_path)\n",
    "                chunk_audios.append(audio)\n",
    "                os.remove(chunk_path)  # Delete chunk file\n",
    "            \n",
    "            # Concatenate and save final audio\n",
    "            final_audio = np.concatenate(chunk_audios)\n",
    "            sf.write(out_path, final_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally run evaluation with `speech_gen_eval`\n",
    "import os\n",
    "from speech_gen_eval.evaluation import speech_gen_eval\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    print(f\"Evaluating {fold}\")\n",
    "    txt_path = os.path.join(\"speech_gen_eval_testsets\", fold, \"test\")\n",
    "    generated_audio = os.path.join(\"xtts\", fold, \"wav\")\n",
    "    original_audio = os.path.join(\"speech_gen_eval_testsets\", fold, \"wav\")\n",
    "    id_mapping = os.path.join(\"speech_gen_eval_testsets\", fold, \"id_mapping\")\n",
    "    speech_gen_eval(\n",
    "        txt_path=txt_path,\n",
    "        generated_audio=generated_audio,\n",
    "        eval_type=\"zero-tts\",\n",
    "        original_audio=original_audio,\n",
    "        mapping_path=id_mapping,\n",
    "        out_path=os.path.join(\"xtts\", fold, \"metrics.yaml\"),\n",
    "        ignore_missing=True,\n",
    "        # extra arguments to write into the metrics.yaml as meta info\n",
    "        model_name=\"XTTSv2\",\n",
    "        dataset=f\"balacoon/speech_gen_eval_testsets/{fold}\",\n",
    "        link=\"https://github.com/coqui-ai/TTS/blob/dev/docs/source/models/xtts.md#single-reference-1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload synthetic audio and metrics to `speech_gen_baselines`,\n",
    "# so it is available on TTSLeaderboard\n",
    "\n",
    "local_dataset = \"xtts\"\n",
    "hf_dataset = \"balacoon/speech_gen_baselines\"\n",
    "hf_subdir = \"zero-tts/xtts\"\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the Hugging Face API\n",
    "api = HfApi()\n",
    "\n",
    "# Upload each fold to the appropriate subdirectory\n",
    "for fold in [\"vctk\", \"daps_celeb\"]:\n",
    "    # Upload wav files\n",
    "    api.upload_folder(\n",
    "        folder_path=os.path.join(local_dataset, fold),\n",
    "        repo_id=hf_dataset,\n",
    "        repo_type=\"dataset\",\n",
    "        path_in_repo=os.path.join(hf_subdir, fold)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech_gen_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
